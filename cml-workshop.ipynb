{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "460e0586-79bd-4cf4-badf-f94c75a6a713",
   "metadata": {},
   "source": [
    "# LLM Workshop\n",
    "\n",
    "In this workshop we will be using a privately hosted LLM model exposed as REST API. The REST API mimics OpenAI REST API specification so we will use several examples throughout the workshop that can be used as a drop in replacement of OpenAI. We will be going through several examples:\n",
    "- Deploying chatGPT like application using streamlit\n",
    "- Using completion API on a notebook\n",
    "- Using completion API with langchain on a notebook\n",
    "- Using langchain to develop Retrieval Augmented Generation on a notebook\n",
    "- Building chatPDF from scratch using streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f642a5c1-348e-4ab7-9607-8bf5812c0367",
   "metadata": {},
   "source": [
    "## Deploying chatGPT like application using streamlit\n",
    "\n",
    "We are not going to develop this from scratch. We will simply deploy the example code as an application in CML. This is to show what to expect and what can be done after we finish the workshop.\n",
    "\n",
    "* Go to examples folder and open `start_chat.py`\n",
    "    * This is the entry point script that we will be calling when the application starts\n",
    "    * You are free to inspect and go through the code but we will not explain it in detail for now.\n",
    "* Open \"Applications\" page from the CML left-hand side menu\n",
    "* Click \"New Application\"\n",
    "* Give it a name as \"Chat[XX]\" with XX is your workshop username (for example ChatUser01)\n",
    "* Create subdomain as \"chat[xx]-app\" (for example chatuser01-app)\n",
    "* Select `start_chat.py` as the script\n",
    "* Select 2vCPU and 4GB memory as the resource profile \n",
    "* Hit \"Create Application\"\n",
    "\n",
    "Wait for the application to be deployed. Once it is deployed and started, you can play around with it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0c4523-4a31-47ec-bd73-5a31a9387ae3",
   "metadata": {},
   "source": [
    "## Using Completion API on a notebook\n",
    "\n",
    "Now that you have seen what you can do with a privately hosted LLM model, we will start learning from the basic: accessing the LLM REST API from a notebook.\n",
    "\n",
    "Start a notebook session and install `openai` client library. We are not using OpenAI service here, we are just using their client library to hit our privately hosted LLM server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eebe2a1-0e41-42f7-bb80-b9ce30c9ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b07ea-754f-4c99-9a54-836a1be8545e",
   "metadata": {},
   "source": [
    "Next we will start with setting up the required environment variables to access our privately hosted LLM server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4211b3e4-463a-4248-9ae7-2192b42cc8a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Modify OpenAI's API key, API base, model accordingly.\n",
    "#os.environ['OPENAI_API_KEY']=\"sk-111111111111111111111111111111111111111111111111\"\n",
    "#os.environ['OPENAI_API_BASE']=os.environ[\"LLM_API_SERVER_BASE\"]\n",
    "openai.api_key = \"sk-111111111111111111111111111111111111111111111111\"\n",
    "openai.api_base = os.environ[\"LLM_API_SERVER_BASE\"]\n",
    "model = \"x\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e716f0f-7cb1-49af-b59a-d3b4fd8da935",
   "metadata": {},
   "source": [
    "It's all set! Now we can try using the `Completion` API. You can refer to the documentation here:\n",
    "- https://platform.openai.com/docs/api-reference/completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "638ea7de-def2-48fe-a1dd-e9a4c7303960",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=conv-1692943828948771584 at 0x7fa8a866afc0> JSON: {\n",
       "  \"id\": \"conv-1692943828948771584\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1692943828,\n",
       "  \"model\": \"Llama-2-13B-chat-GPTQ\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"text\": \"?\\nI'm doing well, thanks for asking! It's always nice to connect with someone new. What brings you here today? Do you have any questions or topics you'd like to discuss? I'm all ears (or eyes, rather)!\",\n",
       "      \"logprobs\": null\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 4,\n",
       "    \"completion_tokens\": 54,\n",
       "    \"total_tokens\": 58\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.Completion.create(\n",
    "            model=model,\n",
    "            temperature=1.31,\n",
    "            top_p=0.14,\n",
    "            repetition_penalty=1.17,\n",
    "            top_k=49,\n",
    "            max_tokens=1000,\n",
    "            prompt=\"Hello How are you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550cbf29-5b3c-4435-9d4c-cd93f61baaef",
   "metadata": {},
   "source": [
    "Our fake OpenAI also allows us to call `ChatCompletion` API which is more suitable for conversational application. More details on the documentation here:\n",
    "- https://platform.openai.com/docs/api-reference/chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "707886e8-f2ed-49b0-aac3-8c196c4ad316",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completions id=chatcmpl-1692763315431015424 at 0x7f207065ba10> JSON: {\n",
       "  \"id\": \"chatcmpl-1692763315431015424\",\n",
       "  \"object\": \"chat.completions\",\n",
       "  \"created\": 1692763315,\n",
       "  \"model\": \"Llama-2-13B-chat-GPTQ\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"Hello there! I'm here to help answer any questions you may have. What would you like to know?\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 34,\n",
       "    \"completion_tokens\": 23,\n",
       "    \"total_tokens\": 57\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            temperature=1.31,\n",
    "            top_p=0.14,\n",
    "            repetition_penalty=1.17,\n",
    "            top_k=49,\n",
    "            max_tokens=1000,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bccd59-4793-40a4-b473-cbf7fb2bda6b",
   "metadata": {},
   "source": [
    "Let's try another one. One of the hardest question humanity has ever encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b90c2c54-3c98-4591-8799-50725f7ba107",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completions id=chatcmpl-1692943842413918976 at 0x7fa8a866a7f0> JSON: {\n",
       "  \"id\": \"chatcmpl-1692943842413918976\",\n",
       "  \"object\": \"chat.completions\",\n",
       "  \"created\": 1692943842,\n",
       "  \"model\": \"Llama-2-13B-chat-GPTQ\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"Based on the current weather forecast, there is a high chance of heavy rain showers throughout the day. To stay dry and comfortable, I would recommend bringing both an umbrella and a raincoat. The umbrella will provide protection from the rain while you are walking or standing, while the raincoat will keep you dry and warm during any prolonged exposure to the elements. Additionally, wearing a raincoat can also help to protect your clothes from getting wet and ruined.\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 55,\n",
       "    \"completion_tokens\": 103,\n",
       "    \"total_tokens\": 158\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            temperature=1.31,\n",
    "            top_p=0.14,\n",
    "            repetition_penalty=1.17,\n",
    "            top_k=49,\n",
    "            max_tokens=1000,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Hello! it's about to rain today. Should I bring an umbrella or a raincoat?\"}\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e7795-06a0-4f38-bdf8-284a81930e7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using completion API with langchain on a notebook\n",
    "\n",
    "Next we are going to integrate with Langchain. It is designed to simplify the creation of applications using large language models. More information on langhchain can be found here:\n",
    "- https://python.langchain.com/docs/get_started/introduction.html\n",
    "\n",
    "Let's start with installing `langchain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d083133a-7e45-492c-9562-62211ef0d731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2812617-fd2c-4614-abaa-c11c926fcf50",
   "metadata": {},
   "source": [
    "Once it's installed we will start with setting up the necessary environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc466e1-9143-40c8-ab51-a71bcdc6063b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-111111111111111111111111111111111111111111111111\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.environ[\"LLM_API_SERVER_BASE\"]\n",
    "model = \"x\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3760d165-75c1-49fb-9fff-dcadfa7dedb6",
   "metadata": {},
   "source": [
    "One key difference here with how we did it previously is we are now using the OpenAI wrapper from langchain. This allows us to use other functionality provided by langchain which we will try more later. For now, let's use one of langchain callbacks feature to stream the generated text even inside a notebook. Notice the `StreamingStdOutCallbackHandler()` being passed as one of the `callbacks` arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6ee4883-ea4a-4a82-bc92-341f7f9ac7b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "            model=model,\n",
    "            temperature=1.31,\n",
    "            top_p=0.14,\n",
    "            max_tokens=1000,\n",
    "            streaming=True, \n",
    "            callbacks=[StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "404d1512-bb4a-4632-a403-709547e0ff92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "Jakarta is a bustling metropolis with a rich history, diverse culture, and modern attractions, from colonial-era landmarks to vibrant street food markets."
     ]
    }
   ],
   "source": [
    "resp = llm(\"tell me about Jakarta in one sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c3d981-8d3c-4a7b-a34f-cb7c64127089",
   "metadata": {},
   "source": [
    "## Using langchain to develop Retrieval Augmented Generation on a notebook\n",
    "\n",
    "We are now going to do something much more interesting. We will use langhchain to allow our LLM to answer questions based on the context given in a supplied document. This approach is also known as Retrieval Augmented Generation (RAG). We will be using a vector database to store the context as for fast retrieval during the Q&A chain.\n",
    "\n",
    "Let's start with installing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bc3821a-3b41-4028-b988-a10735214839",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain chromadb pypdf sentence_transformers pysqlite3-binary tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669e402b-d6d3-43dc-a56e-558e406ef6c7",
   "metadata": {},
   "source": [
    "Similar with what we did previously we are going to set OpenAI endpoint to our privately hosted LLM server. For the embeddings, we will be using OpenAIEmbeddings here. However, since ours is not a real OpenAI API, it is actually using `all-mpnet-base-v2` under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd9c0720-c2a0-4763-b4ab-e8d69da8cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-111111111111111111111111111111111111111111111111\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.environ[\"LLM_API_SERVER_BASE\"]\n",
    "model = \"x\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb55cea6-510e-435e-8021-f305ccb8f6a8",
   "metadata": {},
   "source": [
    "We need to do this to workaround the issue of ChromaDB sqlite dependency. Reference:\n",
    "- https://docs.trychroma.com/troubleshooting#sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b246b223-e8b7-4b21-aefc-77e073aa74d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6587a779-eb15-4bc0-a073-8ac8c581d518",
   "metadata": {},
   "source": [
    "Next, we will be using `WebBaseLoader` to load our context for Q&A. This is just an example, we can also load PDF or Text files. We just need to make sure that we use the right loaders. Once the context is loaded, we will split the text and store it in a vector store. We will be using ChromaDB for the vector store in this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71204e1f-8087-4711-a288-f752d19b478d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://blog.cloudera.com/applying-fine-grained-security-to-apache-spark/\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6d9c4bf-033e-4374-8ab2-7d8937b5e171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfc25724-a44d-4fd5-9cd3-8371e086e461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12541f55-d2bc-4b89-9008-c2056ffb1c79",
   "metadata": {},
   "source": [
    "The content of the web page that we loaded earlier was splitted and converted into vector embeddings and loaded into ChromaDB. To simulate that the vector store works, we will show how we can retrieve information from the vector store using similarity search. When we build our QA chain later on, the chain will perform similarity search on our vector store and provide it as additional context to the LLM. So essentially what the LLM does is \"answering\" the question by summarizing the context given as part of the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0114be9e-4261-4329-92e9-8d3f33782809",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='mixed results.\\xa0 One approach is to use 3rd party tools (such as Privacera)\\xa0 that integrate with Spark. However, it not only increases costs but requires duplication of policies and yet another external tool to manage. Other approaches also fall short by serving as partial solutions to the problem. For example, EMR plus Lake Formation makes a compromise by only providing column level security but not controlling row filtering.', metadata={'language': 'en-US', 'source': 'https://blog.cloudera.com/applying-fine-grained-security-to-apache-spark/', 'title': 'Applying Fine Grained Security to Apache Spark - Cloudera Blog'}), Document(page_content='Fine grained access control (FGAC) with Spark', metadata={'language': 'en-US', 'source': 'https://blog.cloudera.com/applying-fine-grained-security-to-apache-spark/', 'title': 'Applying Fine Grained Security to Apache Spark - Cloudera Blog'}), Document(page_content='Introducing Spark Secure Access Mode\\nStarting with CDP 7.1.7 SP1 (announced earlier this year in March), we introduced a new access mode with Spark that adheres to the centralized FGAC policies defined within SDX. In the coming months we will enhance this to make it even easier with minimal to no code changes in your applications, while being performant and without limiting the Spark APIs used.', metadata={'language': 'en-US', 'source': 'https://blog.cloudera.com/applying-fine-grained-security-to-apache-spark/', 'title': 'Applying Fine Grained Security to Apache Spark - Cloudera Blog'}), Document(page_content='underlying files circumventing more fine grained access that would otherwise limit rows or columns.', metadata={'language': 'en-US', 'source': 'https://blog.cloudera.com/applying-fine-grained-security-to-apache-spark/', 'title': 'Applying Fine Grained Security to Apache Spark - Cloudera Blog'})]\n"
     ]
    }
   ],
   "source": [
    "question = \"What is spark secure access?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bccd9fb-1edb-4d87-b676-25a3191dfa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,retriever=vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbe489cb-78b1-4cb7-ac15-00becf08326d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, \"Spark Secure Access\" refers to a new access mode introduced in CDP 7.1.7 SP1 for Apache Spark, which adheres to centralized Fine Grained Access Control (FGAC) policies defined within SDX. This access mode enables fine-grained access control over data stored in underlying files, without limiting the Spark APIs used and with minimal to no code changes required in applications.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is spark secure access?\"\n",
    "output = qa_chain({\"query\": question},return_only_outputs=True)\n",
    "print(output['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960944df-5f78-472f-a93c-5b18baaee8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
